# -*- coding: utf-8 -*-
"""Test_task_MMK (1).ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1pUwCOz2vRD6dg51LYMJo69w-x0YeQHcY
"""

#!pip install Keras

#!pip install pymorphy2

import keras

#!pip install stop_words

from keras.models import Model
from keras import layers
from keras import Input
from pymorphy2 import MorphAnalyzer
import pickle
import string
from pymorphy2 import MorphAnalyzer
from stop_words import get_stop_words

def preprocess_txt(morpher, line):
#    sw = set(get_stop_words("ru"))
    exclude = set(string.punctuation)
    spls = "".join(i for i in line.strip() if i not in exclude).split()
    spls = [morpher.parse(i.lower())[0].normal_form for i in spls]
#    spls = [i for i in spls if i not in sw and i != ""]
    return spls

path = '/content/sample_data/paraphrases.xml'

from lxml import objectify
import pandas as pd

xml = objectify.parse(path)
root = xml.getroot()
root.getchildren()[1]

root = xml.getroot()
root = root.getchildren()[1]
#<value name="id">1</value>
#<value name="id_1">201</value>
#<value name="id_2">8159</value>
#<value name="text_1">Полицейским разрешат стрелять на поражение по гражданам с травматикой.</value>
#<value name="text_2">Полиции могут разрешить стрелять по хулиганам с травматикой.</value>
#<value name="jaccard">0.65</value>
#<value name="class">0</value>

df = pd.DataFrame(columns=('id','id_1', 'id_2', 'text_1', 'text_2', 'jaccard', 'class'))

for i in range(0,len(root.getchildren())):
    obj = root.getchildren()[i].getchildren()
    row = dict(zip(['id','id_1', 'id_2', 'text_1', 'text_2', 'jaccard', 'class'], [obj[0].text, obj[1].text, obj[2].text, obj[3].text, obj[4].text, obj[5].text, obj[6].text]))
    row_s = pd.Series(row)
    row_s.name = i
    df = df.append(row_s)
df

#Создать словарь
#Ключ: слово
#Значение: порядковый номер

list_text = list(df['text_1']) + list(df['text_2'])

dict_words = {}
i = 0
morpher = MorphAnalyzer()
for text in list_text:
  #разбить предложения на слова
    words = preprocess_txt(morpher, text)
    for word in words:
        if word in dict_words:
          pass
        else:
          dict_words[word] = int(i)
          i += 1
len(dict_words)

text_vocabulary_size = len(dict_words.keys())#10
question_vocabulury_size = len(dict_words.keys())#10000
#answer_vocabulary_size = 500

text_input = Input(shape=(None,), dtype='int32', name='text')

embedded_text = layers.Embedding(
text_vocabulary_size, 64)(text_input)

encoded_text = layers.LSTM(32)(embedded_text)

question_input = Input(shape=(None,),
                      dtype = 'int32',
                      name='question')

embedded_question = layers.Embedding(
text_vocabulary_size, 64)(question_input)

encoded_question = layers.LSTM(32)(embedded_question)

concatenated = layers.concatenate([encoded_text, encoded_question], axis=-1)

answer = layers.Dense(3, activation='softmax')(concatenated)

model = Model([text_input, question_input], answer)
model.compile(optimizer='rmsprop', loss='categorical_crossentropy', metrics=['acc'])

import numpy as np
morpher = MorphAnalyzer()
num_samples = len(df) #длина датасета
max_length = 100

#text = np.random.randint(1, text_vocabulary_size, size=(num_samples, max_length))

#не забудь нули добавлять в НАЧАЛО
text = np.zeros(shape=(num_samples, max_length))
list_text1 = list(df['text_1'])

for n in range(num_samples):
    words = preprocess_txt(morpher, list_text1[n])
    cur_text = text[n]
    #надо из конца в начало
    i = len(words) - 1
    j = len(cur_text) - 1
    while i >= 0:
        cur_text[j] = dict_words[words[i]]
        i -= 1
        j -= 1
print(text)
print(text.shape)

#question = np.random.randint(1, question_vocabulury_size, size=(num_samples, max_length))
question = np.zeros(shape=(num_samples, max_length))
list_text2 = list(df['text_2'])

for n in range(num_samples):
    words = preprocess_txt(morpher, list_text1[n])
    cur_text = question[n]
    #надо из конца в начало
    i = len(words) - 1
    j = len(cur_text) - 1
    while i >= 0:
        cur_text[j] = dict_words[words[i]]
        i -= 1
        j -= 1
print(question)
print(question.shape)



answers = np.zeros(shape=(num_samples, 3))
indices = np.array(df['class']) #-1,0,1
indices2 = []
for i in indices:
  indices2.append(int(i) + 1) 
   
indices = np.array(indices2)
for i, x in enumerate(answers):
    x[indices[i]] = 1

print(answers)
print(answers.shape)


#model.fit([text, question], answers, epochs=10, batch_size=128)

from sklearn.model_selection import train_test_split
#text_train, text_val = train_test_split(
#    text, train_size=0.7, random_state=27, shuffle=True
#)
#question_train, question_val = train_test_split(
#    question, train_size=0.7, random_state=27, shuffle=True
#)

#answers_train, answers_val = train_test_split(
#    answers, train_size=0.7, random_state=27, shuffle=True
#)

training_samples = 6200
validation_samples = 1000
text_train  = text[:training_samples]
question_train = question[:training_samples]
answer_train = answers[:training_samples]
text_val = text[training_samples: training_samples + validation_samples]
question_val = question[training_samples: training_samples + validation_samples]
answer_val = answers[training_samples: training_samples + validation_samples]

history = model.fit([text_train, question_train], answer_train, epochs=3, batch_size=128, validation_data=([text_val,question_val], answer_val))

results = model.evaluate([text_val,question_val], answer_val)

results

y_hat = model.predict([text_val, question_val])

y_hat.shape

from sklearn.metrics import roc_auc_score, roc_curve
pct_auc = roc_auc_score(answer_val[:,0], y_hat[:,0])*100
print("ROC AUC по несхожим фразам: ", pct_auc)

pct_auc = roc_auc_score(answer_val[:,1], y_hat[:,1])*100
print("ROC AUC по схожим фразам: ", pct_auc)

pct_auc = roc_auc_score(answer_val[:,2], y_hat[:,2])*100
print("ROC AUC по идентичным фразам: ", pct_auc)

#нужно взять 2 произвольные фразы, оцифровать их и определить похожесть
#
import numpy as np

#функция 
#вход: передаем на вход 2 фразы
#выход: на выходе строка 
#Классы перефразирования: -1: непарафразы, 0: свободные перефразы, 1: строгие перефразы.

def get_paraphrase_classes(phrase1, phrase2):
    morpher = MorphAnalyzer()
    num_samples = 1 #длина датасета
    max_length = 100

    #не забудь нули добавлять в НАЧАЛО
    text = np.zeros(shape=(num_samples, max_length))

    words = preprocess_txt(morpher, phrase1)
    print(words)
    cur_text = text[0]
    #надо из конца в начало
    i = len(words) - 1
    j = len(cur_text) - 1
    while i >= 0:
    #проверка на наличие слова в словаре
        if words[i] in dict_words.keys(): 
            cur_text[j] = dict_words[words[i]]
        else:
            cur_text[j] = len(dict_words.keys()) + 1    
        i -= 1
        j -= 1
#----------------------------
#не забудь нули добавлять в НАЧАЛО
    question = np.zeros(shape=(num_samples, max_length))

    words = preprocess_txt(morpher, phrase2)
    cur_text = question[0]
    #надо из конца в начало
    i = len(words) - 1
    j = len(cur_text) - 1
    while i >= 0:
    #проверка на наличие слова в словаре
        if words[i] in dict_words.keys(): 
            cur_text[j] = dict_words[words[i]]
        else:
            cur_text[j] = len(dict_words.keys()) + 1    
        i -= 1
        j -= 1

    predictions = model.predict([text, question])
    n = np.argmax(predictions[i,:])
#Paraphrase classes: -1: non-paraphrases, 0: loose paraphrases, 1: strict paraphrases.
#Классы перефразирования: -1: непарафразы, 0: свободные перефразы, 1: строгие перефразы.
    if n == 0:
        return False
    else:
        return True

phrase1 = "Полицейским разрешат стрелять на поражение по гражданам с травматикой."
phrase2 = "Полиции могут разрешить стрелять по хулиганам с травматикой."

paraphrase_class = get_paraphrase_classes(phrase1, phrase2)
if paraphrase_class:
    print("свободные или строгие перефразы")
else:
    print('не являются перефразами')